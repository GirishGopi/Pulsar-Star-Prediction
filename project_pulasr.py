# -*- coding: utf-8 -*-
"""Project-Pulasr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U9zFPB2m_jOCdodOoVi6HHsPyOlviG4H
"""

#Importing packages
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.metrics import classification_report,accuracy_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from imblearn.combine import SMOTEENN
import warnings

#Ignoring warnings
warnings.filterwarnings('ignore')

#Reading the dataset
df=pd.read_csv("/content/drive/MyDrive/Dataset/Pulsar.csv")
df.head()

#Dropping duplicate entries if any
df.drop_duplicates(inplace=True)

#Checking whether the dataset has any null values.
df.isna().sum()

# Understanding the datatypes of the columns.
df.dtypes

#Visualizing the values of the Integrated Profile
plt.subplot(3,2,1)
plt.title("Mean Integrated")
plt.hist(x=df["Mean_Integrated"])
plt.subplot(3,2,2)
plt.title("Integrated SD")
plt.hist(x=df["SD"])
plt.subplot(3,2,5)
plt.title("Integrated EK")
plt.hist(x=df["EK"])
plt.subplot(3,2,6)
plt.title("Integrated Skewness")
plt.hist(x=df["Skewness"])
plt.suptitle("Integrated Profile values")

plt.show()

#Visualizing the values of the DM-SNR Curve
plt.subplot(3,2,1)
plt.title("Mean of DM-SNR Curve")
plt.hist(x=df["Mean_DMSNR_Curve"])
plt.subplot(3,2,2)
plt.title("SD of DM-SNR Curve")
plt.hist(x=df["SD_DMSNR_Curve"])
plt.subplot(3,2,5)
plt.title("EK of DM-SNR Curve")
plt.hist(x=df["EK_DMSNR_Curve"])
plt.subplot(3,2,6)
plt.title("Skewness of DM-SNR Curve")
plt.hist(x=df["Skewness_DMSNR_Curve"])
plt.suptitle("DM-SNR Crve values")
plt.show()

#Visualizing the values of 'Class' to determine the imbalancy of the dataset.
plt.title("Prediction Class")
plt.pie(df["Class"].value_counts(),labels=["0","1"])
plt.show()

"""It is clear that the dataset is highly imbalanced in which 0, corresponding to 'Negative', occurs the most."""

# Generating the heat map of the correlation of the columns of the dataset.
sns.heatmap(df.corr(),annot=True)

"""It may be observed that almost all the attributes are actually correlated with the prediction class. So, all the features have to be considered.
Alos, since there is only eight features, no dimensionality reduction is required.
"""

#Splitting the dataset into attributes and label class.
X=df.iloc[:,:-1]
y=df.iloc[:,-1]

#Scaling X values
sc=StandardScaler()
X=sc.fit_transform(X)

#Splitting the data into train data and test data
X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=1,test_size=0.3)

#Fixing classifiers using a for loop
knn=KNeighborsClassifier()
sv=SVC(random_state=1)
gnb=GaussianNB()
dtc=DecisionTreeClassifier(criterion="entropy")
rfc=RandomForestClassifier(n_estimators=50,criterion="entropy",random_state=1)
abc=AdaBoostClassifier(random_state=1)
acc=[]
models=[knn,sv,gnb,dtc,rfc,abc]
for model in models:
  print(model.__class__.__name__)
  model.fit(X_train,y_train)
  y_pred=model.predict(X_test)
  print("Accuracy:",accuracy_score(y_test,y_pred))
  print(classification_report(y_test,y_pred))
  acc.append(accuracy_score(y_test,y_pred))
  print("*"*100)

#Comparing the accuracy of the models and finding the highly accurate one.
accuracy_before_resampling=pd.DataFrame({"Model":["KNearestNeighbors","SVC","GaussianNB","DecisionTreeClassifier","RandomForestClassifier","AdaBoostClassifier"],"Accuracy":acc})

#Creating a function for highlighting the cell with maximum accuracy
def highlight(x):
  if x==max(accuracy_before_resampling["Accuracy"]):
    return "background-color:blue;color:white;"
  else:
    return ""

#Comapring accuracies and finding the model with maximum accuracy using the 'highlight' function.
accuracy_before_resampling.style.applymap(highlight)

"""It is observed that the RandomForestClassifier gives the maximum accuracy."""

#Hyper Tuning RandomForestClassifier
#params= {'n_estimators': [30,40,50],'min_samples_split':[2,5,10],'max_features':[None,'sqrt'],'criterion':['entropy','log_loss'],'random_state':[1]}
#clf=GridSearchCV(RandomForestClassifier(),params,cv=5,scoring='accuracy')
#clf.fit(X_train,y_train)
#print(clf.best_params_)

#Evaluating the performance of the hyper tuned RandomForestClassifier
rfc=RandomForestClassifier(criterion="entropy",max_features=None,min_samples_split=10,n_estimators=30,random_state=1)
rfc.fit(X_train,y_train)
y_pred=rfc.predict(X_test)
acc_aht=accuracy_score(y_test,y_pred)
print(accuracy_score(y_test,y_pred))
print(classification_report(y_test,y_pred))

#Creating a function for highlighting the cell with maximum accuracy
def highlight(x):
  if x==max(accuracy_comparission.iloc[0]):
    return "background-color:blue;color:white;"
  else:
    return ""

#Comparing the accuracies of RandomForestClassifier models before and after hypertuning.
accuracy_comparission=pd.DataFrame([[accuracy_before_resampling.iloc[4,1],acc_aht]],columns=["Accuracy Before Hypertuning","Accuracy After Hypertuning"])
accuracy_comparission.style.applymap(highlight)

"""It is clear that the accuracy of the RandomForestClassifier model has increased, little a bit, after hypertuning."""

#Updating the accuracy of the RandomForestClassifier model with that of the hypertuned model.
acc[4]=acc_aht

"""Now, we go for resampling.
Since the dataset is highly imbalanced, neigther over sampling nor under samling would give us a better result.
So, we go for the middle-line of the above preocedures. That is, doing both over sampling and under sampling.
We use SMOTEENN for this purpose.
"""

#Resampling using SMOTEENN
sm=SMOTEENN(random_state=1)
X_train,y_train=sm.fit_resample(X_train,y_train)

#Fitting models to the resampled dataset and comparing the performance.
acc_rs=[]
for model in models:
  print(model.__class__.__name__)
  model.fit(X_train,y_train)
  y_pred=model.predict(X_test)
  print(classification_report(y_test,y_pred))
  acc_rs.append(accuracy_score(y_test,y_pred))
  print("*"*100)

#Comparing the accuracies of the models before and after resampling.
accuracy=pd.DataFrame({"Model":["KNearestNeighbors","SVC","GaussianNB","DecisionTreeClassifier","RandomForestClassifier","AdaBoostClassifier"],"Accuracy Before Resampling":acc,"Accuracy After Resampling":acc_rs})
accuracy

"""It is observed that the accuracy of the models has reduced after resampling.
Also, it may be concluded that the hyper tuned RandomForestClassifier performs better as compared to others, on the original dataset, with an accuracy of around 0.980074.
"""